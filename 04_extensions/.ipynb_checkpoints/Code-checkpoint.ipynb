{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains experiments for:\n",
    "\n",
    "* Loss functions\n",
    "* Learning rate decay\n",
    "* Weight initialization\n",
    "* Optimizers\n",
    "* Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `lincoln` imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lincoln\n",
    "from lincoln.layers import Dense\n",
    "from lincoln.losses import SoftmaxCrossEntropy, MeanSquaredError\n",
    "from lincoln.optimizers import Optimizer, SGD, SGDMomentum\n",
    "from lincoln.activations import Sigmoid, Tanh, Linear, ReLU\n",
    "from lincoln.network import NeuralNetwork\n",
    "from lincoln.train import Trainer\n",
    "from lincoln.utils import mnist\n",
    "from lincoln.utils.np_utils import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist.init() Use the function once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = mnist.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = len(y_train)\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode\n",
    "num_labels = len(y_train)\n",
    "train_labels = np.zeros((num_labels, 10))\n",
    "for i in range(num_labels):\n",
    "    train_labels[i][y_train[i]] = 1\n",
    "\n",
    "num_labels = len(y_test)\n",
    "test_labels = np.zeros((num_labels, 10))\n",
    "for i in range(num_labels):\n",
    "    test_labels[i][y_test[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "some_digit = X_train[45]\n",
    "some_digit_image = some_digit.reshape(28, 28)\n",
    "plt.subplot(1, 1, 1)\n",
    "image = some_digit_image\n",
    "plt.imshow(image.astype(np.uint8))\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Demos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale data to mean 0, variance 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train - np.mean(X_train), X_test - np.mean(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(X_train), np.max(X_train), np.min(X_test), np.max(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train / np.std(X_train), X_test / np.std(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(X_train), np.max(X_train), np.min(X_test), np.max(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_model(model, test_set):\n",
    "    return print(f'''The model validation accuracy is: {np.equal(np.argmax(model.forward(test_set, inference=True), axis=1), y_test).sum() * 100.0 / test_set.shape[0]:.2f}%''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=89, \n",
    "                  activation=Tanh()),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Sigmoid())],\n",
    "            loss = MeanSquaredError(normalize=False), \n",
    "seed=20190119)\n",
    "\n",
    "trainer = Trainer(model, SGD(0.1))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "            epochs = 50,\n",
    "            eval_every = 10,\n",
    "            seed=20190119,\n",
    "            batch_size=60);\n",
    "print()\n",
    "calc_accuracy_model(model, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: even if we normalize the outputs of a classification model with mean squared error loss, it still doesn't help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=89, \n",
    "                  activation=Tanh()),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Sigmoid())],\n",
    "            loss = MeanSquaredError(normalize=True), \n",
    "seed=20190119)\n",
    "\n",
    "trainer = Trainer(model, SGD(0.1))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "            epochs = 50,\n",
    "            eval_every = 10,\n",
    "            seed=20190119,\n",
    "            batch_size=60);\n",
    "\n",
    "calc_accuracy_model(model, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason is that we should be using softmax cross entropy loss!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=89, \n",
    "                  activation=Sigmoid()),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear())],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20190119)\n",
    "\n",
    "trainer = Trainer(model, SGD(0.1))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "            epochs = 130,\n",
    "            eval_every = 1,\n",
    "            seed=20190119,\n",
    "            batch_size=60);\n",
    "print()\n",
    "calc_accuracy_model(model, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying ReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=89, \n",
    "                  activation=ReLU()),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear())],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20190119)\n",
    "\n",
    "trainer = Trainer(model, SGD(0.1))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "            epochs = 50,\n",
    "            eval_every = 10,\n",
    "            seed=20190119,\n",
    "            batch_size=60);\n",
    "print()\n",
    "calc_accuracy_model(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=89, \n",
    "                  activation=Tanh()),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear())],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20190119)\n",
    "\n",
    "trainer = Trainer(model, SGD(0.1))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "            epochs = 50,\n",
    "            eval_every = 10,\n",
    "            seed=20190119,\n",
    "            batch_size=60);\n",
    "print()\n",
    "calc_accuracy_model(model, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=89, \n",
    "                  activation=Sigmoid()),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear())],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20190119)\n",
    "\n",
    "optim = SGDMomentum(0.1, momentum=0.9)\n",
    "\n",
    "trainer = Trainer(model, SGDMomentum(0.1, momentum=0.9))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "            epochs = 50,\n",
    "            eval_every = 1,\n",
    "            seed=20190119,\n",
    "            batch_size=60);\n",
    "\n",
    "calc_accuracy_model(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=89, \n",
    "                  activation=Tanh()),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear())],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20190119)\n",
    "\n",
    "optim = SGD(0.1)\n",
    "\n",
    "optim = SGDMomentum(0.1, momentum=0.9)\n",
    "\n",
    "trainer = Trainer(model, SGDMomentum(0.1, momentum=0.9))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "            epochs = 50,\n",
    "            eval_every = 10,\n",
    "            seed=20190119,\n",
    "            batch_size=60);\n",
    "\n",
    "calc_accuracy_model(model, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=89, \n",
    "                  activation=Tanh()),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear())],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20190119)\n",
    "\n",
    "optimizer = SGDMomentum(0.15, momentum=0.9, final_lr = 0.05, decay_type='linear')\n",
    "\n",
    "trainer = Trainer(model, optimizer)\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "            epochs = 50,\n",
    "            eval_every = 10,\n",
    "            seed=20190119,\n",
    "            batch_size=60);\n",
    "\n",
    "calc_accuracy_model(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=89, \n",
    "                  activation=Tanh()),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear())],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20190119)\n",
    "\n",
    "optimizer = SGDMomentum(0.2, \n",
    "                        momentum=0.9, \n",
    "                        final_lr = 0.05, \n",
    "                        decay_type='exponential')\n",
    "\n",
    "trainer = Trainer(model, optimizer)\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "            epochs = 50,\n",
    "            eval_every = 10,\n",
    "            seed=20190119,\n",
    "            batch_size=60);\n",
    "\n",
    "calc_accuracy_model(model, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing weight init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=89, \n",
    "                  activation=Tanh(),\n",
    "                  weight_init=\"glorot\"),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear(),\n",
    "                  weight_init=\"glorot\")],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20190119)\n",
    "\n",
    "optimizer = SGDMomentum(0.15, momentum=0.9, final_lr = 0.05, decay_type='linear')\n",
    "\n",
    "trainer = Trainer(model, optimizer)\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "       epochs = 50,\n",
    "       eval_every = 10,\n",
    "       seed=20190119,\n",
    "           batch_size=60,\n",
    "           early_stopping=True);\n",
    "\n",
    "calc_accuracy_model(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=89, \n",
    "                  activation=Tanh(),\n",
    "                  weight_init=\"glorot\"),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear(),\n",
    "                  weight_init=\"glorot\")],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20190119)\n",
    "\n",
    "trainer = Trainer(model, SGDMomentum(0.2, momentum=0.9, final_lr = 0.05, decay_type='exponential'))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "       epochs = 50,\n",
    "       eval_every = 10,\n",
    "       seed=20190119,\n",
    "           batch_size=60,\n",
    "           early_stopping=True);\n",
    "\n",
    "calc_accuracy_model(model, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=89, \n",
    "                  activation=Tanh(),\n",
    "                  weight_init=\"glorot\",\n",
    "                  dropout=0.8),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear(),\n",
    "                  weight_init=\"glorot\")],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20190119)\n",
    "\n",
    "trainer = Trainer(model, SGDMomentum(0.2, momentum=0.9, final_lr = 0.05, decay_type='exponential'))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "       epochs = 50,\n",
    "       eval_every = 10,\n",
    "       seed=20190119,\n",
    "           batch_size=60,\n",
    "           early_stopping=True);\n",
    "\n",
    "calc_accuracy_model(model, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning, with and without Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=178, \n",
    "                  activation=Tanh(),\n",
    "                  weight_init=\"glorot\",\n",
    "                  dropout=0.8),\n",
    "            Dense(neurons=46, \n",
    "                  activation=Tanh(),\n",
    "                  weight_init=\"glorot\",\n",
    "                  dropout=0.8),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear(),\n",
    "                  weight_init=\"glorot\")],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20190119)\n",
    "\n",
    "trainer = Trainer(model, SGDMomentum(0.2, momentum=0.9, final_lr = 0.05, decay_type='exponential'))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "       epochs = 100,\n",
    "       eval_every = 10,\n",
    "       seed=20190119,\n",
    "           batch_size=60,\n",
    "           early_stopping=True);\n",
    "\n",
    "calc_accuracy_model(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=178, \n",
    "                  activation=Tanh(),\n",
    "                  weight_init=\"glorot\"),\n",
    "            Dense(neurons=46, \n",
    "                  activation=Tanh(),\n",
    "                  weight_init=\"glorot\"),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear(),\n",
    "                  weight_init=\"glorot\")],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20190119)\n",
    "\n",
    "trainer = Trainer(model, SGDMomentum(0.2, momentum=0.9, final_lr = 0.05, decay_type='exponential'))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "       epochs = 100,\n",
    "       eval_every = 10,\n",
    "       seed=20190119,\n",
    "           batch_size=60,\n",
    "           early_stopping=True);\n",
    "\n",
    "calc_accuracy_model(model, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlcourse",
   "language": "python",
   "name": "dlcourse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
